{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vibration Data Processing\n",
    "\n",
    "### Industrial pump\n",
    "\n",
    "The object of this notebook is the processing of vibration data acquired from a sensor monitoring an industrial pump in order to do identify the machine state from a measure.  \n",
    "\n",
    "First, the data is processed to produce features that can be used in a ML model.\n",
    "  \n",
    "Then a Principla Component Analysis is run on the dataset to reduce the feature space.  \n",
    "  \n",
    "Finally a SVM model is trained to predict the state of the machine based on the vibration recording and tested on some samples.  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function # for Python 2.7 compatibility\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import operator\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as sg\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from math import floor, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: './Input_data/DataSet/project_pump.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-08860175c2d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the csv data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./Input_data/DataSet/project_pump.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"[;]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#df = pd.read_csv('./Input_data/DataSet/project_fan.csv', header=None, sep=\"[;]\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    993\u001b[0m                                  ' \"c\", \"python\", or' ' \"python-fwf\")'.format(\n\u001b[1;32m    994\u001b[0m                                      engine=engine))\n\u001b[0;32m--> 995\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, **kwds)\u001b[0m\n\u001b[1;32m   1983\u001b[0m         f, handles = _get_handle(f, mode, encoding=self.encoding,\n\u001b[1;32m   1984\u001b[0m                                  \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1985\u001b[0;31m                                  memory_map=self.memory_map)\n\u001b[0m\u001b[1;32m   1986\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/io/common.pyc\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;31m# Python 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;31m# Python 3 and encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: './Input_data/DataSet/project_pump.csv'"
     ]
    }
   ],
   "source": [
    "# Load the csv data\n",
    "\n",
    "df = pd.read_csv('./Input_data/DataSet/project_pump.csv', header=None, sep=\"[;]\")\n",
    "#df = pd.read_csv('./Input_data/DataSet/project_fan.csv', header=None, sep=\"[;]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the measures loaded as string in the last column as list of float\n",
    "df[4] = df[4].apply(lambda x : map(float,x.strip(\"[]\").split(\", \")))\n",
    "\n",
    "# Convert the timestamps into ms elapsed since the origin\n",
    "df[0] = df[0].apply(lambda x : x-df[0][0])\n",
    "\n",
    "#Verify that there is no null data in the set\n",
    "df.isnull().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display part of the data loaded\n",
    "\n",
    "def display_pd(some_df):\n",
    "    display(HTML(some_df.to_html()))\n",
    "    \n",
    "display_pd(df.head(5)) # special display to see all columns\n",
    "print('{} samples'.format(df.shape[0])) # Dimensions of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the constant baseline on the 50 first samples\n",
    "mean_list = []\n",
    "for i in range(50):\n",
    "    mean_list.append(np.mean(df[4].iloc[i]))\n",
    "baseline = np.mean(mean_list)\n",
    "print('baseline measure : {} mV'.format(baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Signal processing functions\n",
    "\n",
    "def rem_baseline(signal):\n",
    "    ''' Remove the baseline signal from sample measure\n",
    "    '''\n",
    "    signal -= baseline\n",
    "    return signal\n",
    "    \n",
    "def calcFFT(epoch):\n",
    "    ''' Calculates the FFT of the epoch signal. Removes the DC component.\n",
    "    '''\n",
    "    M = len(epoch)\n",
    "    w = np.kaiser(M, 0.11)\n",
    "    spectrum = np.fft.fft(w*epoch, axis=0)\n",
    "    spectrum = np.abs(spectrum)\n",
    "    spectrum[0]=0 # set the DC component to zero\n",
    "    #S /= S.sum()\n",
    "\n",
    "    return spectrum\n",
    "\n",
    "def calcWelch(epoch):\n",
    "    \"\"\"Calculate the PSD using the Welch method.\n",
    "    \"\"\"\n",
    "    len(epoch)\n",
    "    samp_rate = 48188\n",
    "    f, epoch = sg.welch(epoch, samp_rate)\n",
    "    return f, epoch\n",
    "\n",
    "def calcPSDPeak(epoch):\n",
    "    \"\"\"Calculate the PSD using the Welch method.\n",
    "    \"\"\"\n",
    "    len(epoch)\n",
    "    f, epoch = sg.welch(epoch, fs)\n",
    "    return f, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Signal:\n",
    "    def __init__(self, signal):\n",
    "        self.signal = signal\n",
    "        self.signal = self.signal.apply(rem_baseline)\n",
    "    def process(self, processor):\n",
    "        processor.start_object(self.signal)\n",
    "        return processor.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ProcessorSelector():\n",
    "    def get_processor(self, transf):\n",
    "        if transf == 'FFT':\n",
    "            return FFT_Processor()\n",
    "        elif transf == 'PSD':\n",
    "            return PSD_Processor()\n",
    "\n",
    "select = ProcessorSelector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SignalProcessor:\n",
    "    def process(self, signal, transf):\n",
    "        processor = select.get_processor(transf)\n",
    "        return signal.process(processor)\n",
    "    \n",
    "class FFT_Processor:\n",
    "    def __init_(self):\n",
    "        self._current_obj = None\n",
    "    def start_object(self, signal):\n",
    "        self._current_obj = signal\n",
    "    def process(self):\n",
    "        return self._current_obj.apply(calcFFT)\n",
    "\n",
    "class PSD_Processor:\n",
    "    def __init_(self):\n",
    "        self._current_obj = None\n",
    "    def start_object(self, signal):\n",
    "        self._current_obj = signal\n",
    "    def process(self):\n",
    "        f_list, psd_list = [], []\n",
    "        for i in range(len(self._current_obj)):\n",
    "            f, psd = calcWelch(self._current_obj[i])\n",
    "            f_list.append(f)\n",
    "            psd_list.append(psd)\n",
    "        f = pd.Series(f_list)\n",
    "        psd = pd.Series(psd_list)\n",
    "        return f, psd #self._current_obj.apply(calcWelch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "signal = Signal(df[4].copy(deep=True))\n",
    "processor = SignalProcessor()\n",
    "\n",
    "fft = processor.process(signal, 'FFT')\n",
    "f, psd = processor.process(signal, 'PSD')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the analysis done, the selected features are the following:  \n",
    "- Peaks values from the spectrum,\n",
    "- Peaks values from the PSD,\n",
    "- Mean,\n",
    "- Variance.\n",
    "\n",
    "For each sample, a total of 12 peaks are collected from the spectrum.\n",
    "The peaks values for the spectrum are determined automatically for each sample in 5 ranges of frequency:\n",
    "[8000, 12500], [3000, 6000], [0, 1000], [17000, 18500], [19000, 22000]\n",
    "If there is no peak above the value predefined in each range compare to the rest of the signal, the value is set to 0 for this sample. \n",
    "\n",
    "Likewise, a total of 6 peaks are collected from the PSD, in 6 ranges of frequency:\n",
    "[0, 2000], [2000, 6000], [6000, 10000], [10000, 15000], [15000, 19000], [19000, 23000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the features\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "features = ['fft_pk1', 'fft_pk2', 'fft_pk3', 'fft_pk4', 'fft_pk5', 'pk_PSD', 'mean', 'var']\n",
    "\n",
    "\n",
    "peaks = df[4].copy(deep=True)\n",
    "\n",
    "features_df = pd.DataFrame(np.zeros((df.shape[0], 19)))\n",
    "\n",
    "for feat in features:\n",
    "    if feat[:2] == 'ff':\n",
    "        pk_nb = [0, 4, 3, 2, 1, 2][int(feat[-1])]-sum([0, 4, 3, 2, 1, 2][:int(feat[-1])-1])\n",
    "        freq_range = [[0,400], [3000,6000],[9000,12500], [17000,18500], [19000,22000]][int(feat[-1])-1]\n",
    "        \n",
    "        if feat[-1] == '0':\n",
    "            percent, dist, prom, mini = [89, 15, 0.9, 600]\n",
    "        elif feat[-1] == '4':\n",
    "            percent, dist, prom, mini = [89, 15, 6000, 600]\n",
    "        else:\n",
    "            percent, dist, prom, mini = [95, 200, 0.9, 600]\n",
    "        \n",
    "        for i in range(df[4].shape[0]):\n",
    "            inf = int(freq_range[0]/float((df[3].iloc[i]/df[1].iloc[i])))\n",
    "            sup = int(freq_range[1]/float((df[3].iloc[i]/df[1].iloc[i])))\n",
    "            \n",
    "            #if feat[-1] == '0':\n",
    "            #    peaks[i] = np.array([])\n",
    "            try:\n",
    "                indexes, _ = sg.find_peaks(fft[i][inf:sup], height = max(mini,np.percentile(fft[i][inf:sup], percent)), distance=dist, prominence=prom)\n",
    "                #peaks[i] = np.concatenate([peaks[i],np.zeros(pk_nb)])\n",
    "                for j in range(min(pk_nb, len(indexes))):\n",
    "                    features_df.iloc[i,j+[0, 4, 3, 2, 1, 2][int(feat[-1])-1]] = fft[i][indexes[j]]\n",
    "                    #peaks[i][-pk_nb+j] = fft[i][indexes[j]]\n",
    "            except:\n",
    "                continue\n",
    "                #peaks[i] = np.concatenate([peaks[i],np.zeros(pk_nb)])\n",
    "        \n",
    "    elif feat == 'pk_PSD':\n",
    "        list_indexes = []\n",
    "        percent, dist, prom, mini = [85, 0, 0, 0.0001]\n",
    "        for i in range(df[4].shape[0]):\n",
    "            #peaks[i] = np.array([])\n",
    "            indexes, _ = sg.find_peaks(psd[i], height = max(mini, np.percentile(psd[i], percent)))\n",
    "            #peak_freq = [0,0,0,0,0,0]\n",
    "            for j in range(1, 7):\n",
    "                freq_range = [0, 2000, 6000, 10000, 15000, 19000, 23000]\n",
    "                try:\n",
    "                    features_df.iloc[i, j+12] = max(psd[i][indexes[(f.iloc[i][indexes]>freq_range[j-1]) & (f.iloc[i][indexes]<freq_range[j])]])\n",
    "                    #peak_freq[j] = max(psd[i][indexes[(f.iloc[i][indexes]>freq_range[j-1]) & (f.iloc[i][indexes]<freq_range[j])]])\n",
    "                except:\n",
    "                    continue\n",
    "            #peaks[i] = np.concatenate([peaks[i],peak_freq])\n",
    "            list_indexes.append(len(indexes))\n",
    "    elif feat == 'mean':\n",
    "        for i in range(df[4].shape[0]):\n",
    "            features_df.iloc[i, 17] = np.mean(df[4].iloc[i])\n",
    "            #peaks[i] = np.concatenate([peaks[i],[np.mean(df[4].iloc[i])]])\n",
    "    elif feat == 'var':\n",
    "        for i in range(df[4].shape[0]):\n",
    "            features_df.iloc[i, 18] = np.var(df[4].iloc[i])\n",
    "            #peaks[i] = np.concatenate([peaks[i],[np.var(df[4].iloc[i])]])\n",
    "\n",
    "\n",
    "features_df.fillna(0)\n",
    "# Rescale the features to (0,1)\n",
    "for i in range(19):\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    x = features_df.iloc[:,i].values.reshape(1,-1)\n",
    "    x = np.array(features_df.iloc[:,i].values).reshape(x.shape[1],1)\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    features_df.iloc[:,i] = pd.DataFrame(x_scaled)\n",
    "    \n",
    "\n",
    "#peaks.values.shape\n",
    "#print(peaks.values[350])\n",
    "#min_max_scaler = MinMaxScaler()\n",
    "#peaks_scaled = min_max_scaler.fit_transform(peaks.values.reshape(-1, 1))\n",
    "#df = pandas.DataFrame(peaks_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "nb_comp = 7\n",
    "pca = PCA(n_components=nb_comp)\n",
    "principalComponents = pca.fit_transform(features_df)\n",
    "\n",
    "print('PCA Components Explained Variance')\n",
    "print('Components Variance: {}'.format(pca.explained_variance_ratio_))\n",
    "print('Total Explained Variance for {} components: {}%'.format(nb_comp, np.round(100.*sum(pca.explained_variance_ratio_),2)))\n",
    "\n",
    "principal_df = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['pc1', 'pc2', 'pc3', 'pc4', 'pc5', 'pc6', 'pc7'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the PCA analysis, 97.7% of the variance of the data can be represented by 7 components.  \n",
    "We will use this PC reduction for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to test this set of features for finding out to which phase a sample belongs.\n",
    "To do so, we define the phases and select samples that we annotate for training a model.\n",
    "\n",
    "We will use a SVM model for this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Annotate the samples\n",
    "steps = [181,538,709,735]\n",
    "categories = np.zeros(df.shape[0])\n",
    "for step in steps:\n",
    "    for j in range(step,df.shape[0]):\n",
    "        categories[j] += 1\n",
    "\n",
    "\n",
    "\n",
    "principal_df['cat'] = categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the test set\n",
    "test_samples = [50, 75, 100, 200, 300, 400, 500, 600, 650, 709]\n",
    "test_set_df = principal_df.iloc[test_samples].copy(deep=True)\n",
    "test_set_df = test_set_df.drop(['cat'], axis = 1)\n",
    "#Create the train set\n",
    "train_set_df = principal_df.drop([50, 75, 100, 200, 300, 400, 500, 600, 650, 709], axis=0).copy(deep=True)\n",
    "print(test_set_df.shape, train_set_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "X = train_set_df[['pc1', 'pc2', 'pc3', 'pc4', 'pc5', 'pc6', 'pc7']]\n",
    "Y = train_set_df['cat']\n",
    "clf = svm.SVC()\n",
    "clf.fit(X, Y)\n",
    "\n",
    "pred = clf.predict(test_set_df)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach provide some interesting resutls already.  \n",
    "With a sensor placed on the same machine, we can verify what is the regime of the machine and probably detect some undesired situations.  \n",
    "This requires to have data annotated beforehand.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
